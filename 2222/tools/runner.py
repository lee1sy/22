import os, h5py, torch, pickle, faiss, gc
import numpy as np
from torch import nn
import torch.nn.functional as F
from torch.optim import Adam, lr_scheduler
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

class GaussianUDGLoss(nn.Module):
    def __init__(self, margin=0.5, lambda_gcl=0.1, lambda_pml=0.1, device='cuda'):
        super().__init__()
        self.margin, self.lambda_gcl, self.lambda_pml = margin, lambda_gcl, lambda_pml
        self.device = device
        self.geo_proj = nn.Linear(128, 6).to(device)

    def forward(self, global_des, batch_dict, output_dict, nNeg):
        loss_wtl = self._compute_lazy_triplet(global_des, nNeg)
        gaussians = batch_dict['gaussians']
        fused_feat = output_dict.get('fused_feat')
        valid_mask = output_dict.get('valid_mask')
        loss_gcl = self._compute_gcl(fused_feat, gaussians, valid_mask) if fused_feat is not None else 0
        sampled_feats = output_dict.get('sampled_visual_feats')
        loss_pml = self._compute_pml(sampled_feats, gaussians, valid_mask) if sampled_feats is not None else 0
        return loss_wtl + self.lambda_gcl * loss_gcl + self.lambda_pml * loss_pml

    def _safe_distance(self, x1, x2):
        diff = x1 - x2
        return torch.sqrt(torch.sum(diff * diff, dim=-1) + 1e-8)

    def _compute_lazy_triplet(self, global_des, nNeg):
        neg_des, pos_des, query_des = torch.split(global_des, [nNeg, 1, 1], dim=0)
        d_pos = self._safe_distance(query_des.expand(nNeg, -1), pos_des.expand(nNeg, -1))
        d_neg = self._safe_distance(query_des.expand(nNeg, -1), neg_des)
        loss = torch.clamp(d_pos - d_neg + self.margin, min=0.0)
        mask = (loss > 0).float()
        return (loss * mask).sum() / (mask.sum() + 1e-6)

    def _compute_gcl(self, fused_feat, gaussians, mask):
        geo_gt_input = torch.cat([gaussians[..., :3], gaussians[..., 4:7]], dim=-1)
        geo_gt_norm = torch.norm(geo_gt_input, dim=-1, keepdim=True)
        # å¦‚æœ norm ä¸º 0ï¼Œä¼šäº§ç”Ÿ NaNï¼Œéœ€è¦å¤„ç†
        if (geo_gt_norm < 1e-8).any():
            # å¯¹äºå…¨é›¶å‘é‡ï¼Œä½¿ç”¨å•ä½å‘é‡
            geo_gt = geo_gt_input / (geo_gt_norm + 1e-8)
            # å°†å…¨é›¶ä½ç½®è®¾ä¸º [1,0,0,0,0,0]
            zero_mask = (geo_gt_norm.squeeze(-1) < 1e-8)
            geo_gt[zero_mask, 0] = 1.0
            geo_gt[zero_mask, 1:] = 0.0
        else:
            geo_gt = geo_gt_input / geo_gt_norm
        
        feat_proj_input = self.geo_proj(fused_feat)
        feat_proj_norm = torch.norm(feat_proj_input, dim=-1, keepdim=True)
        if (feat_proj_norm < 1e-8).any():
            feat_proj = feat_proj_input / (feat_proj_norm + 1e-8)
            zero_mask = (feat_proj_norm.squeeze(-1) < 1e-8)
            feat_proj[zero_mask, 0] = 1.0
            feat_proj[zero_mask, 1:] = 0.0
        else:
            feat_proj = feat_proj_input / feat_proj_norm
        
        dist = 1 - torch.clamp(F.cosine_similarity(feat_proj, geo_gt, dim=-1), -1.0 + 1e-7, 1.0 - 1e-7)
        return (dist * mask.float()).sum() / (mask.sum() + 1e-6)

    def _compute_pml(self, sampled_feats, gaussians, mask):
        weight = (gaussians[..., 11] * torch.norm(gaussians[..., 4:7], dim=-1)).detach()
        vis_norm = torch.norm(sampled_feats, dim=-1)
        return (vis_norm * weight * mask.float()).sum() / (mask.sum() + 1e-6)

class Trainer:
    def __init__(self, model, train_loader, whole_train_loader, whole_val_set, whole_val_loader,
                 device, num_epochs, ckpt_dir, cache_dir, log_dir, lr, step_size, gamma, margin, freeze_visual=True):
        self.model, self.device = model, device
        self.train_loader, self.whole_train_loader = train_loader, whole_train_loader
        self.whole_val_loader, self.whole_val_set = whole_val_loader, whole_val_set
        self.ckpt_dir, self.cache_dir, self.num_epochs = ckpt_dir, cache_dir, num_epochs
        self.criterion = GaussianUDGLoss(margin=margin, device=device).to(device)
        
        # ä¸ºä¸åŒæ¨¡å—è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
        # è§†è§‰ç›¸å…³ï¼ˆproj_layer èåˆè§†è§‰å’ŒLiDARï¼‰ï¼šè¾ƒå°å­¦ä¹ ç‡
        # LiDARç›¸å…³ï¼ˆspconv_enc, uncert_netï¼‰ï¼šè¾ƒå¤§å­¦ä¹ ç‡
        # èšåˆå±‚ï¼ˆvladï¼‰ï¼šä¸­ç­‰å­¦ä¹ ç‡
        visual_params = []
        lidar_params = []
        fusion_params = []
        vlad_params = []
        
        # å¤„ç†è§†è§‰ç¼–ç å™¨å†»ç»“/è§£å†»
        if not freeze_visual:
            # è§£å†»è§†è§‰ç¼–ç å™¨ï¼Œç”¨äºå¾®è°ƒ
            for name, param in model.named_parameters():
                if 'visual_enc' in name:
                    param.requires_grad = True
            print("âš ï¸  è§†è§‰ç¼–ç å™¨å·²è§£å†»ï¼Œå°†ç”¨æå°å­¦ä¹ ç‡å¾®è°ƒ")
        else:
            # ç¡®ä¿è§†è§‰ç¼–ç å™¨å†»ç»“
            for name, param in model.named_parameters():
                if 'visual_enc' in name:
                    param.requires_grad = False
            print("âœ… è§†è§‰ç¼–ç å™¨å·²å†»ç»“ï¼ˆä½¿ç”¨é¢„è®­ç»ƒç‰¹å¾ï¼‰")
        
        for name, param in model.named_parameters():
            if not param.requires_grad:
                continue
            if 'visual_enc' in name:
                # è§†è§‰ç¼–ç å™¨ï¼šæå°å­¦ä¹ ç‡ï¼ˆå¦‚æœè§£å†»ï¼‰
                visual_params.append(param)
            elif 'proj_layer' in name:
                # èåˆå±‚ï¼šè¾ƒå°å­¦ä¹ ç‡ï¼ˆè§†è§‰ç‰¹å¾å·²ç»é¢„è®­ç»ƒå¥½ï¼‰
                fusion_params.append(param)
            elif 'spconv_enc' in name or 'uncert_net' in name:
                # LiDARç›¸å…³ï¼šè¾ƒå¤§å­¦ä¹ ç‡
                lidar_params.append(param)
            elif 'vlad' in name:
                # èšåˆå±‚ï¼šä¸­ç­‰å­¦ä¹ ç‡
                vlad_params.append(param)
            else:
                # å…¶ä»–å‚æ•°ï¼šé»˜è®¤å­¦ä¹ ç‡
                lidar_params.append(param)
        
        # è®¾ç½®ä¸åŒå­¦ä¹ ç‡ï¼šLiDAR > VLAD > Fusion > Visual
        # å»ºè®®æ¯”ä¾‹ï¼šLiDAR:VLAD:Fusion:Visual = 1:0.5:0.1:0.01
        optimizer_params = [
            {'params': lidar_params, 'lr': lr, 'name': 'lidar'},  # LiDAR: 1x
            {'params': vlad_params, 'lr': lr * 0.5, 'name': 'vlad'},  # VLAD: 0.5x
            {'params': fusion_params, 'lr': lr * 0.1, 'name': 'fusion'},  # Fusion: 0.1x
        ]
        
        # å¦‚æœè§†è§‰ç¼–ç å™¨è§£å†»ï¼Œæ·»åŠ æå°å­¦ä¹ ç‡
        if len(visual_params) > 0:
            optimizer_params.append({'params': visual_params, 'lr': lr * 0.01, 'name': 'visual'})  # Visual: 0.01x
        
        # è¿‡æ»¤æ‰ç©ºåˆ—è¡¨
        optimizer_params = [p for p in optimizer_params if len(p['params']) > 0]
        
        self.optimizer = Adam(optimizer_params, lr=lr)
        self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=step_size, gamma=gamma)
        
        # æ‰“å°å­¦ä¹ ç‡è®¾ç½®
        print("\n" + "="*80)
        print("å­¦ä¹ ç‡è®¾ç½®:")
        for param_group in self.optimizer.param_groups:
            print(f"  {param_group.get('name', 'unknown')}: lr={param_group['lr']:.6f}, å‚æ•°æ•°é‡={len(param_group['params'])}")
        print("="*80 + "\n")
        self.writer = SummaryWriter(log_dir)

    def train(self):
        if torch.cuda.device_count() > 1: self.model = nn.DataParallel(self.model)
        self.model.to(self.device)
        for epoch in range(1, self.num_epochs + 1):
            self.build_cache()
            self.model.train()
            epoch_losses = []
            pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")
            for i, (input_dict, nNeg) in enumerate(pbar):
                if input_dict is None: continue
                batch = {k: v.squeeze(0).to(self.device) if isinstance(v, torch.Tensor) else v for k, v in input_dict.items()}
                self.optimizer.zero_grad()
                out = self.model(batch)
                emb = out['embedding']
                
                # è¯¦ç»†æ£€æŸ¥ NaN æ¥æº
                if torch.isnan(emb).any() or torch.isinf(emb).any():
                    print(f"\nâŒ NaN/Inf in embedding at iteration {i}")
                    print(f"  æ£€æŸ¥æ¨¡å‹è¾“å‡º...")
                    if out.get('fused_feat') is not None and torch.isnan(out['fused_feat']).any():
                        print(f"  âŒ fused_feat åŒ…å« NaN")
                        # æ£€æŸ¥ fused_feat çš„æ¥æº
                        print(f"    æ£€æŸ¥è¾“å…¥...")
                        if torch.isnan(batch['gaussians']).any():
                            print(f"    âŒ gaussians è¾“å…¥åŒ…å« NaN")
                    if out.get('sampled_visual_feats') is not None and torch.isnan(out['sampled_visual_feats']).any():
                        print(f"  âŒ sampled_visual_feats åŒ…å« NaN")
                    if out.get('offsets') is not None and torch.isnan(out['offsets']).any():
                        print(f"  âŒ offsets åŒ…å« NaN")
                        # æ£€æŸ¥ offsets çš„æ¥æº
                        print(f"    æ£€æŸ¥ UncertaintyNet è¾“å…¥...")
                        if torch.isnan(batch['gaussians']).any():
                            print(f"    âŒ gaussians è¾“å…¥åŒ…å« NaN")
                        # æ£€æŸ¥æ¨¡å‹å‚æ•°
                        for name, param in self.model.named_parameters():
                            if 'uncert_net' in name and torch.isnan(param).any():
                                print(f"    âŒ UncertaintyNet å‚æ•° {name} åŒ…å« NaNï¼ˆå¯èƒ½æ˜¯æ¢¯åº¦çˆ†ç‚¸ï¼‰")
                    print(f"  è·³è¿‡æ­¤è¿­ä»£ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦åŒ…å« NaN")
                    continue
                
                # æ£€æŸ¥ embedding norm
                emb_norm = torch.norm(emb, dim=1, keepdim=True)
                if (emb_norm < 1e-8).any():
                    print(f"\nâš ï¸  WARNING: embedding norm æ¥è¿‘ 0 at iteration {i}, norm_min={emb_norm.min().item():.6f}")
                
                # å®‰å…¨å½’ä¸€åŒ–
                emb_normalized = emb / (emb_norm + 1e-8)
                
                # å†æ¬¡æ£€æŸ¥
                if torch.isnan(emb_normalized).any() or torch.isinf(emb_normalized).any():
                    print(f"\nâŒ NaN/Inf after normalization at iteration {i}")
                    print(f"  è·³è¿‡æ­¤è¿­ä»£")
                    continue
                
                loss = self.criterion(emb_normalized, batch, out, nNeg[0])
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"\nâŒ NaN/Inf loss at iteration {i}")
                    # æ£€æŸ¥æŸå¤±å‡½æ•°çš„å„ä¸ªç»„æˆéƒ¨åˆ†
                    gaussians = batch['gaussians']
                    fused_feat = out.get('fused_feat')
                    valid_mask = out.get('valid_mask')
                    
                    # æ£€æŸ¥ _compute_gcl çš„è¾“å…¥
                    if fused_feat is not None:
                        geo_gt_input = torch.cat([gaussians[..., :3], gaussians[..., 4:7]], dim=-1)
                        geo_gt_norm = torch.norm(geo_gt_input, dim=-1)
                        if (geo_gt_norm < 1e-8).any():
                            print(f"  âŒ geo_gt_input åŒ…å«å…¨é›¶å‘é‡ï¼Œnorm_min={geo_gt_norm.min().item():.6f}")
                        if torch.isnan(geo_gt_input).any():
                            print(f"  âŒ geo_gt_input åŒ…å« NaN")
                    
                    print(f"  è·³è¿‡æ­¤è¿­ä»£")
                    continue
                loss.backward()
                
                # è¯¦ç»†çš„æ¢¯åº¦è¿½æº¯
                has_nan_grad = False
                max_grad_norm = 0.0
                grad_info = {}  # æŒ‰æ¨¡å—åˆ†ç»„ç»Ÿè®¡
                
                for name, param in self.model.named_parameters():
                    if param.grad is not None:
                        if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                            print(f"\n{'='*80}")
                            print(f"ğŸ” æ¢¯åº¦è¿½æº¯ - NaN/Inf æ¢¯åº¦")
                            print(f"{'='*80}")
                            print(f"å‚æ•°: {name}")
                            print(f"å½¢çŠ¶: {param.shape}")
                            nan_count = torch.isnan(param.grad).sum().item() if torch.isnan(param.grad).any() else 0
                            inf_count = torch.isinf(param.grad).sum().item() if torch.isinf(param.grad).any() else 0
                            print(f"NaNæ•°é‡: {nan_count} || Infæ•°é‡: {inf_count}")
                            print(f"{'='*80}\n")
                            has_nan_grad = True
                            param.grad.zero_()
                        else:
                            param_grad_norm = param.grad.data.norm(2).item()
                            max_grad_norm = max(max_grad_norm, param_grad_norm)
                            
                            # æŒ‰æ¨¡å—åˆ†ç»„ç»Ÿè®¡
                            module_name = name.split('.')[0] if '.' in name else name
                            if module_name not in grad_info:
                                grad_info[module_name] = {'count': 0, 'total_norm': 0.0, 'max_norm': 0.0, 'params': []}
                            grad_info[module_name]['count'] += 1
                            grad_info[module_name]['total_norm'] += param_grad_norm
                            grad_info[module_name]['max_norm'] = max(grad_info[module_name]['max_norm'], param_grad_norm)
                            grad_info[module_name]['params'].append((name, param_grad_norm))
                            
                            # å¦‚æœå•ä¸ªå‚æ•°çš„æ¢¯åº¦è¿‡å¤§ï¼Œä¹Ÿæ¸…é›¶
                            if param_grad_norm > 100.0:
                                print(f"\n{'='*80}")
                                print(f"ğŸ” æ¢¯åº¦è¿½æº¯ - å•ä¸ªå‚æ•°æ¢¯åº¦è¿‡å¤§")
                                print(f"{'='*80}")
                                print(f"å‚æ•°: {name}")
                                print(f"å½¢çŠ¶: {param.shape}")
                                print(f"æ¢¯åº¦èŒƒæ•°: {param_grad_norm:.2f}")
                                print(f"å‚æ•°å€¼èŒƒå›´: [{param.data.min().item():.6f}, {param.data.max().item():.6f}]")
                                print(f"æ¢¯åº¦å€¼èŒƒå›´: [{param.grad.data.min().item():.6f}, {param.grad.data.max().item():.6f}]")
                                print(f"{'='*80}\n")
                                param.grad.zero_()
                                has_nan_grad = True
                
                if has_nan_grad:
                    continue
                
                # è®¡ç®—æ€»æ¢¯åº¦èŒƒæ•°ï¼ˆä¸è£å‰ªï¼Œåªè§‚å¯Ÿï¼‰
                # æ³¨æ„ï¼šæ¢¯åº¦èŒƒæ•°æœ¬èº«ä¸æ˜¯é—®é¢˜ï¼Œå…³é”®æ˜¯å®é™…å‚æ•°æ›´æ–°é‡ = lr * grad_norm
                # å½“å‰ lr = 1e-5ï¼Œæ‰€ä»¥ grad_norm = 12.28 æ—¶ï¼Œå®é™…æ›´æ–°é‡ = 0.0001228ï¼Œè¿™æ˜¯å®‰å…¨çš„
                # ä½†å¦‚æœæ¢¯åº¦å‘ˆæŒ‡æ•°å¢é•¿ï¼ˆ1 â†’ 10 â†’ 100 â†’ 1000ï¼‰ï¼Œé‚£å°±æ˜¯æ¢¯åº¦çˆ†ç‚¸
                total_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=float('inf'))
                
                # è®¡ç®—å®é™…å‚æ•°æ›´æ–°é‡ï¼ˆæ›´åˆç†çš„åˆ¤æ–­æ ‡å‡†ï¼‰
                actual_update = self.optimizer.param_groups[0]['lr'] * total_norm
                
                # åªåœ¨æ¢¯åº¦èŒƒæ•°å¾ˆå¤§ï¼ˆ> 50ï¼‰æˆ–å®é™…æ›´æ–°é‡å¾ˆå¤§ï¼ˆ> 0.01ï¼‰æ—¶æ‰“å°è­¦å‘Š
                if total_norm > 50.0 or actual_update > 0.01:
                    print(f"\n{'='*80}")
                    print(f"âš ï¸  æ¢¯åº¦è§‚å¯Ÿ - æ€»æ¢¯åº¦èŒƒæ•°è¾ƒå¤§")
                    print(f"{'='*80}")
                    print(f"æ€»æ¢¯åº¦èŒƒæ•°: {total_norm:.2f}")
                    print(f"æœ€å¤§å•å‚æ•°æ¢¯åº¦èŒƒæ•°: {max_grad_norm:.2f}")
                    print(f"å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
                    print(f"å®é™…å‚æ•°æ›´æ–°é‡: {actual_update:.6f} (lr * grad_norm)")
                    print(f"è¯´æ˜: å®é™…æ›´æ–°é‡ < 0.001 é€šå¸¸æ˜¯å®‰å…¨çš„ï¼Œ> 0.01 éœ€è¦å…³æ³¨")
                    print(f"\næŒ‰æ¨¡å—ç»Ÿè®¡:")
                    # æŒ‰æ€»æ¢¯åº¦èŒƒæ•°æ’åº
                    sorted_modules = sorted(grad_info.items(), key=lambda x: x[1]['total_norm'], reverse=True)
                    for module_name, info in sorted_modules[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                        print(f"  {module_name}:")
                        print(f"    å‚æ•°æ•°é‡: {info['count']}")
                        print(f"    æ€»æ¢¯åº¦èŒƒæ•°: {info['total_norm']:.2f}")
                        print(f"    æœ€å¤§å•å‚æ•°æ¢¯åº¦èŒƒæ•°: {info['max_norm']:.2f}")
                        # æ˜¾ç¤ºè¯¥æ¨¡å—ä¸­æ¢¯åº¦æœ€å¤§çš„å‚æ•°
                        top_params = sorted(info['params'], key=lambda x: x[1], reverse=True)[:3]
                        for param_name, param_norm in top_params:
                            print(f"      - {param_name}: {param_norm:.2f}")
                    print(f"{'='*80}\n")
                
                self.optimizer.step()
                
                # æ£€æŸ¥å‚æ•°æ˜¯å¦å˜æˆ NaN
                for name, param in self.model.named_parameters():
                    if torch.isnan(param).any():
                        print(f"ERROR: å‚æ•° {name} å˜æˆ NaNï¼Œè®­ç»ƒå¯èƒ½å·²æŸå")
                        # å°è¯•æ¢å¤ï¼šå°† NaN å‚æ•°è®¾ä¸º 0
                        with torch.no_grad():
                            param.data = torch.nan_to_num(param.data, nan=0.0, posinf=0.0, neginf=0.0)
                epoch_losses.append(loss.item())
                pbar.set_postfix(loss=f"{loss.item():.4f}")
            self.scheduler.step()
            self.save_ckpt(epoch)
            self.validate(epoch)

    def build_cache(self):
        self.model.eval()
        path = os.path.join(self.cache_dir, "feat_cache.hdf5")
        nan_count = 0
        with torch.no_grad(), h5py.File(path, 'w') as h5:
            ds_len = len(self.whole_train_loader.dataset)
            feat_set = h5.create_dataset('features', [ds_len, 256], dtype=np.float32)
            ptr = 0
            for batch in tqdm(self.whole_train_loader, desc="Building Cache"):
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
                emb = self.model(batch)['embedding']
                
                # æ£€æŸ¥ NaN/Inf
                if torch.isnan(emb).any() or torch.isinf(emb).any():
                    nan_count += 1
                    emb = torch.zeros_like(emb)
                
                # å®‰å…¨å½’ä¸€åŒ–
                emb_norm = torch.norm(emb, dim=1, keepdim=True)
                emb = emb / (emb_norm + 1e-8)
                
                # å†æ¬¡æ£€æŸ¥
                if torch.isnan(emb).any() or torch.isinf(emb).any():
                    emb = torch.zeros_like(emb)
                
                bs = emb.shape[0]
                feat_set[ptr : ptr + bs] = emb.cpu().numpy()
                ptr += bs
        if nan_count > 0:
            print(f"WARNING: Found NaN/Inf in {nan_count} batches during build_cache")
        gc.collect(); torch.cuda.empty_cache()

    @torch.no_grad()
    def validate(self, epoch):
        self.model.eval()
        all_feats = []
        nan_count = 0
        for batch in tqdm(self.whole_val_loader, desc="Validating"):
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            emb = self.model(batch)['embedding']
            
            # æ£€æŸ¥ NaN/Inf
            if torch.isnan(emb).any() or torch.isinf(emb).any():
                nan_count += 1
                emb = torch.zeros_like(emb)
            
            # å®‰å…¨å½’ä¸€åŒ–
            emb_norm = torch.norm(emb, dim=1, keepdim=True)
            emb = emb / (emb_norm + 1e-8)
            
            # å†æ¬¡æ£€æŸ¥
            if torch.isnan(emb).any() or torch.isinf(emb).any():
                emb = torch.zeros_like(emb)
            
            all_feats.append(emb.cpu().numpy())
        
        feats = np.concatenate(all_feats, axis=0)
        
        # æ£€æŸ¥ feats ä¸­çš„ NaN
        if np.isnan(feats).any() or np.isinf(feats).any():
            print(f"WARNING: Found NaN/Inf in feats, replacing with zeros")
            feats = np.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0)
        
        if nan_count > 0:
            print(f"WARNING: Found NaN/Inf in {nan_count} batches during validate")
        
        db_f, q_f = feats[:self.whole_val_set.num_db], feats[self.whole_val_set.num_db:]
        if db_f.shape[0] == 0 or q_f.shape[0] == 0:
            print(f"ERROR: Empty database or query features")
            return
        index = faiss.IndexFlatL2(256)
        index.add(db_f.astype('float32'))
        _, preds = index.search(q_f.astype('float32'), 20)
        gt = self.whole_val_set.getPositives()
        for k in [1, 5, 10]:
            if len(q_f) == 0:
                acc = 0.0
            else:
                acc = sum(np.any(np.in1d(preds[i, :k], gt[i])) for i in range(len(q_f))) / len(q_f) * 100
            print(f"Recall@{k}: {acc:.2f}%")
            self.writer.add_scalar(f"Val/Recall@{k}", acc, epoch)

    def save_ckpt(self, epoch):
        sd = self.model.module.state_dict() if isinstance(self.model, nn.DataParallel) else self.model.state_dict()
        torch.save({'epoch': epoch, 'net': sd}, os.path.join(self.ckpt_dir, f"GS_PR_epoch_{epoch}.pth.tar"))

class Evaluator:
    def __init__(self, model, test_loader, test_set, device):
        self.model, self.test_loader, self.test_set, self.device = model, test_loader, test_set, device

    @torch.no_grad()
    def full_evaluation(self, ckpt_path):
        ckpt = torch.load(ckpt_path, map_location='cpu')
        sd = ckpt['net']
        if list(sd.keys())[0].startswith('module.'): sd = {k[7:]: v for k, v in sd.items()}
        self.model.load_state_dict(sd)
        self.model.to(self.device).eval()
        all_feats = []
        for batch in tqdm(self.test_loader, desc="Testing"):
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            all_feats.append(F.normalize(self.model(batch)['embedding'], p=2, dim=1).cpu().numpy())
        feats = np.concatenate(all_feats, axis=0)
        db_f, q_f = feats[:self.test_set.num_db], feats[self.test_set.num_db:]
        index = faiss.IndexFlatL2(256)
        index.add(db_f.astype('float32'))
        dists, preds = index.search(q_f.astype('float32'), 50)
        gt = self.test_set.getPositives()
        for k in [1, 5, 10]:
            acc = sum(np.any(np.in1d(preds[i, :k], gt[i])) for i in range(len(q_f))) / len(q_f) * 100
            print(f"Test Recall@{k}: {acc:.2f}%")
        self._compute_pr_and_f1(dists, preds, gt)

    def _compute_pr_and_f1(self, dists, preds, gt):
        min_dists = dists[:, 0]
        thresholds = np.unique(np.sort(min_dists))[::len(min_dists)//200]
        precisions, recalls = [], []
        total_gt = len([g for g in gt if len(g) > 0])
        for th in tqdm(thresholds, desc="PR Curve"):
            tp, fp = 0, 0
            for i in range(len(min_dists)):
                if min_dists[i] < th:
                    if np.any(np.in1d(preds[i, 0], gt[i])): tp += 1
                    else: fp += 1
            p = tp / (tp + fp) if (tp + fp) > 0 else 1.0
            r = tp / total_gt if total_gt > 0 else 0
            precisions.append(p)
            recalls.append(r)
        precisions, recalls = np.array(precisions), np.array(recalls)
        f1 = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)
        print(f"Max F1-score: {np.max(f1):.4f}")
        return precisions, recalls, np.max(f1)