# è®­ç»ƒé—®é¢˜åˆ†æï¼šNaN Loss å’Œ Recall ä¸º 0

## é—®é¢˜æè¿°

1. **NaN Loss é—®é¢˜**ï¼šè®­ç»ƒæ—¶é¢‘ç¹å‡ºç° NaN lossï¼Œå¯¼è‡´éƒ¨åˆ†è¿­ä»£è¢«è·³è¿‡
2. **Recall ä¸º 0 é—®é¢˜**ï¼šç¬¬ä¸€è½®è®­ç»ƒçš„ recall@1/5/10 éƒ½ä¸º 0ï¼ˆ**æ­£å¸¸æ¨¡å‹ç¬¬ä¸€è½® recall ä¸åº”ä½äº 60%**ï¼‰

## âœ… æ•°æ®åŠ è½½æ£€æŸ¥ç»“æœ

è¿è¡Œ `tools/check_data_loading.py` åç¡®è®¤ï¼š
- âœ… Info æ–‡ä»¶æ­£ç¡®åŠ è½½ï¼ˆ22103 ä¸ªæ ·æœ¬ï¼‰
- âœ… é«˜æ–¯ç‚¹äº‘æ•°æ®æ­£ç¡®åŠ è½½ï¼ˆä¸æ˜¯å…¨é›¶ï¼Œæ‰€æœ‰æ ·æœ¬éƒ½æœ‰ 4096 ä¸ªæœ‰æ•ˆç‚¹ï¼‰
- âœ… æ¨¡å‹å‰å‘ä¼ æ’­æ­£å¸¸ï¼ˆembedding æœ‰æ•ˆï¼Œæ²¡æœ‰ NaN/Infï¼‰

**ç»“è®º**ï¼šæ•°æ®åŠ è½½æ²¡æœ‰é—®é¢˜ï¼Œé—®é¢˜å‡ºåœ¨è®­ç»ƒæµç¨‹ä¸­ã€‚

## ğŸ” å…³é”®é—®é¢˜åˆ†æ

### é—®é¢˜ 1: build_cache() ä¸­çš„ NaN å¤„ç†

**ä½ç½®**ï¼š`tools/runner.py:87-100`

```python
def build_cache(self):
    self.model.eval()
    path = os.path.join(self.cache_dir, "feat_cache.hdf5")
    with torch.no_grad(), h5py.File(path, 'w') as h5:
        ds_len = len(self.whole_train_loader.dataset)
        feat_set = h5.create_dataset('features', [ds_len, 256], dtype=np.float32)
        ptr = 0
        for batch in tqdm(self.whole_train_loader, desc="Building Cache"):
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            emb = F.normalize(self.model(batch)['embedding'], p=2, dim=1)  # âš ï¸ é—®é¢˜ç‚¹
            bs = emb.shape[0]
            feat_set[ptr : ptr + bs] = emb.cpu().numpy()
            ptr += bs
```

**æ½œåœ¨é—®é¢˜**ï¼š
- å¦‚æœ `self.model(batch)['embedding']` å…¨ä¸º 0ï¼Œ`F.normalize` ä¼šäº§ç”Ÿ NaN
- å¦‚æœ embedding åŒ…å« NaNï¼Œä¼šè¢«ç›´æ¥å†™å…¥ cache
- Cache ä¸­çš„ NaN ç‰¹å¾ä¼šå¯¼è‡´ triplet æ„å»ºå¤±è´¥
- è¿™ä¼šå¯¼è‡´è®­ç»ƒæ•°æ®æ— æ•ˆï¼Œè¿›è€Œå¯¼è‡´ recall ä¸º 0

### é—®é¢˜ 2: validate() ä¸­çš„ NaN å¤„ç†

**ä½ç½®**ï¼š`tools/runner.py:102-118`

```python
@torch.no_grad()
def validate(self, epoch):
    self.model.eval()
    all_feats = []
    for batch in tqdm(self.whole_val_loader, desc="Validating"):
        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
        all_feats.append(F.normalize(self.model(batch)['embedding'], p=2, dim=1).cpu().numpy())  # âš ï¸ é—®é¢˜ç‚¹
    feats = np.concatenate(all_feats, axis=0)
    # ... æ£€ç´¢è®¡ç®—
```

**æ½œåœ¨é—®é¢˜**ï¼š
- å¦‚æœ embedding å…¨ä¸º 0 æˆ–åŒ…å« NaNï¼Œ`F.normalize` ä¼šäº§ç”Ÿ NaN
- NaN ç‰¹å¾ä¼šå¯¼è‡´ faiss æ£€ç´¢å¼‚å¸¸
- è¿™ä¼šå¯¼è‡´ recall ä¸º 0

### é—®é¢˜ 3: F.normalize åœ¨å…¨é›¶å‘é‡ä¸Šçš„è¡Œä¸º

**æ ¸å¿ƒé—®é¢˜**ï¼šPyTorch çš„ `F.normalize` åœ¨å…¨é›¶å‘é‡ä¸Šä¼šäº§ç”Ÿ NaNï¼š

```python
import torch.nn.functional as F
x = torch.zeros(1, 256)
y = F.normalize(x, p=2, dim=1)
print(y)  # è¾“å‡º: tensor([[nan, nan, ..., nan]])
```

### é—®é¢˜ 4: NetVLADLoupe å¯èƒ½è¾“å‡ºå…¨é›¶

**ä½ç½®**ï¼š`modules/GS.py:132-149`

å¦‚æœ mask å¯¼è‡´æ‰€æœ‰ activation ä¸º 0ï¼Œæˆ–è€…ä¸­é—´è®¡ç®—äº§ç”Ÿå…¨é›¶ï¼Œæœ€ç»ˆ embedding å¯èƒ½å…¨ä¸º 0ã€‚

## ğŸ”§ ä¿®å¤æ–¹æ¡ˆ

### 1. ä¿®å¤ F.normalize åœ¨å…¨é›¶å‘é‡ä¸Šçš„é—®é¢˜

åˆ›å»ºå®‰å…¨çš„å½’ä¸€åŒ–å‡½æ•°ï¼š

```python
def safe_normalize(x, p=2, dim=1, eps=1e-8):
    norm = torch.norm(x, p=p, dim=dim, keepdim=True)
    return x / (norm + eps)
```

### 2. ä¿®å¤ build_cache()

æ·»åŠ  NaN æ£€æŸ¥å’Œä¿®å¤ï¼š

```python
def build_cache(self):
    self.model.eval()
    path = os.path.join(self.cache_dir, "feat_cache.hdf5")
    with torch.no_grad(), h5py.File(path, 'w') as h5:
        ds_len = len(self.whole_train_loader.dataset)
        feat_set = h5.create_dataset('features', [ds_len, 256], dtype=np.float32)
        ptr = 0
        for batch in tqdm(self.whole_train_loader, desc="Building Cache"):
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            emb = self.model(batch)['embedding']
            
            # æ£€æŸ¥ NaN/Inf
            if torch.isnan(emb).any() or torch.isinf(emb).any():
                print(f"WARNING: NaN/Inf in embedding during build_cache, replacing with zeros")
                emb = torch.zeros_like(emb)
            
            # å®‰å…¨å½’ä¸€åŒ–
            emb_norm = torch.norm(emb, dim=1, keepdim=True)
            emb = emb / (emb_norm + 1e-8)
            
            # å†æ¬¡æ£€æŸ¥
            if torch.isnan(emb).any() or torch.isinf(emb).any():
                print(f"WARNING: NaN/Inf after normalization, replacing with zeros")
                emb = torch.zeros_like(emb)
            
            bs = emb.shape[0]
            feat_set[ptr : ptr + bs] = emb.cpu().numpy()
            ptr += bs
```

### 3. ä¿®å¤ validate()

æ·»åŠ  NaN æ£€æŸ¥å’Œä¿®å¤ï¼š

```python
@torch.no_grad()
def validate(self, epoch):
    self.model.eval()
    all_feats = []
    for batch in tqdm(self.whole_val_loader, desc="Validating"):
        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
        emb = self.model(batch)['embedding']
        
        # æ£€æŸ¥ NaN/Inf
        if torch.isnan(emb).any() or torch.isinf(emb).any():
            print(f"WARNING: NaN/Inf in embedding during validate, replacing with zeros")
            emb = torch.zeros_like(emb)
        
        # å®‰å…¨å½’ä¸€åŒ–
        emb_norm = torch.norm(emb, dim=1, keepdim=True)
        emb = emb / (emb_norm + 1e-8)
        
        # å†æ¬¡æ£€æŸ¥
        if torch.isnan(emb).any() or torch.isinf(emb).any():
            print(f"WARNING: NaN/Inf after normalization, replacing with zeros")
            emb = torch.zeros_like(emb)
        
        all_feats.append(emb.cpu().numpy())
    
    feats = np.concatenate(all_feats, axis=0)
    
    # æ£€æŸ¥ feats ä¸­çš„ NaN
    if np.isnan(feats).any() or np.isinf(feats).any():
        print(f"WARNING: NaN/Inf in feats, replacing with zeros")
        feats = np.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0)
    
    db_f, q_f = feats[:self.whole_val_set.num_db], feats[self.whole_val_set.num_db:]
    # ... åç»­æ£€ç´¢ä»£ç 
```

### 4. ä¿®å¤è®­ç»ƒå¾ªç¯ä¸­çš„ normalize

```python
out = self.model(batch)
emb = out['embedding']

# æ£€æŸ¥ NaN/Inf
if torch.isnan(emb).any() or torch.isinf(emb).any():
    print(f"Skipping NaN/Inf embedding at iteration {i}")
    continue

# å®‰å…¨å½’ä¸€åŒ–
emb_norm = torch.norm(emb, dim=1, keepdim=True)
emb_normalized = emb / (emb_norm + 1e-8)

# å†æ¬¡æ£€æŸ¥
if torch.isnan(emb_normalized).any() or torch.isinf(emb_normalized).any():
    print(f"Skipping NaN/Inf normalized embedding at iteration {i}")
    continue

loss = self.criterion(emb_normalized, batch, out, nNeg[0])
```

### 5. ä¿®å¤ NetVLADLoupe

ç¡®ä¿è¾“å‡ºä¸ä¼šå…¨ä¸º 0ï¼š

```python
def forward(self, x, mask=None):
    B, N, D = x.shape
    activation = torch.matmul(x, self.cluster_weights)
    activation = self.norm1(activation)
    activation = F.softmax(activation, dim=-1)
    if mask is not None:
        activation = activation * mask.unsqueeze(-1).to(x.dtype)
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç‚¹
        valid_count = mask.sum(dim=-1, keepdim=True).float()
        if (valid_count < 1).any():
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç‚¹ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
            activation = torch.ones_like(activation) / self.cluster_size
    
    a_sum = activation.sum(-2, keepdim=True)
    a = a_sum * self.cluster_weights2
    vlad = torch.matmul(activation.transpose(1, 2), x)
    vlad = vlad.transpose(1, 2) - a
    
    # å®‰å…¨å½’ä¸€åŒ–
    vlad_norm = torch.norm(vlad, dim=1, keepdim=True)
    vlad = vlad / (vlad_norm + 1e-8)
    
    vlad = vlad.reshape(B, -1)
    vlad_norm2 = torch.norm(vlad, dim=1, keepdim=True)
    vlad = vlad / (vlad_norm2 + 1e-8)
    
    vlad = torch.matmul(vlad, self.hidden1_weights)
    if self.gating:
        vlad = self.gating(vlad)
    
    # æœ€ç»ˆå®‰å…¨å½’ä¸€åŒ–
    vlad_norm3 = torch.norm(vlad, dim=1, keepdim=True)
    vlad = vlad / (vlad_norm3 + 1e-8)
    
    return vlad
```

## æ€»ç»“

**æ ¹æœ¬åŸå› **ï¼š
1. `F.normalize` åœ¨å…¨é›¶å‘é‡ä¸Šäº§ç”Ÿ NaN
2. `build_cache()` å’Œ `validate()` æ²¡æœ‰æ£€æŸ¥ NaNï¼Œå¯¼è‡´ NaN ç‰¹å¾è¢«ä½¿ç”¨
3. NaN ç‰¹å¾å¯¼è‡´æ£€ç´¢å¤±è´¥ï¼Œrecall ä¸º 0

**ä¿®å¤ä¼˜å…ˆçº§**ï¼š
1. **é«˜ä¼˜å…ˆçº§**ï¼šä¿®å¤ `build_cache()` å’Œ `validate()` ä¸­çš„ NaN å¤„ç†
2. **ä¸­ä¼˜å…ˆçº§**ï¼šä¿®å¤è®­ç»ƒå¾ªç¯ä¸­çš„ normalize
3. **ä½ä¼˜å…ˆçº§**ï¼šä¿®å¤ `NetVLADLoupe` ä¸­çš„æ•°å€¼ç¨³å®šæ€§
